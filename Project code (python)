import re
import nltk
import csv
from sklearn.model_selection import train_test_split


# function to process tweets. This works according to rules in the research paper.
def processTweet(tweet):

    # Convert the entire tweet to lower case
    tweet = tweet.lower()
    # Convert www.* or https?://* to URL
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', 'URL', tweet)
    # Convert @username to AT_USER
    tweet = re.sub('@[^\s]+', 'AT_USER', tweet)
    # Remove additional white spaces
    tweet = re.sub('[\s]+', ' ', tweet)
    # Replace #word with word
    tweet = re.sub(r'#([^\s]+)', r'\1', tweet)
    # trim
    tweet = tweet.strip('\'"')
    return tweet
# end of function processTweets.

# start replaceTwoOrMore
def replaceTwoOrMore(s):
    # look for 2 or more repetitions of character and replace with the character itself
    pattern = re.compile(r"(.)\1{1,}", re.DOTALL)
    return pattern.sub(r"\1\1", s)
# end of function replaceTwoOrMore.

# start getStopWordList
def getStopWordList(stopWordListFileName):
    # read the stopwords file and build a list
    stopWords = []
    stopWords.append('AT_USER')
    stopWords.append('URL')

    fp = open(stopWordListFileName, 'r')
    line = fp.readline()
    while line:
        word = line.strip()
        stopWords.append(word)
        line = fp.readline()
    fp.close()
    return stopWords


# end

# start getfeatureVector
def getFeatureVector(tweet, stopWords):
    featureVector = []
    # split tweet into words
    words = tweet.split()
    for w in words:
        # replace two or more with two occurrences
        w = replaceTwoOrMore(w)
        # strip punctuation
        w = w.strip('\'"?,.')
        # check if the word stats with an alphabet
        val = re.search(r"^[a-zA-Z][a-zA-Z0-9]*$", w)
        # ignore if it is a stop word
        if (w in stopWords or val is None):
            continue
        else:
            featureVector.append(w.lower())
    return featureVector


# end

# start extract_features
def extract_features(tweet):
    tweet_words = set(tweet)
    features = {}
    for word in featureList:
        features['contains(%s)' % word] = (word in tweet_words)
    return features


# end

# Read the tweets one by one and process it
# fp = open('sampleTweets.txt', 'r')
# line = fp.readline()
# initialize stopWords
# stopWords = []

# st = open('stopwords.txt', 'r')
# stopWords = getStopWordList('stopwords.txt')


# Read the tweets one by one and process it
inpTweets = csv.reader(open('demonetization-dataset.csv'))
stopWords = getStopWordList('stopwords.txt')
featureList = []
# Get tweet words
tweets = []
i=0

for row in inpTweets:
    tweet = row[2]
    if row[20] in (None, ""):
        continue
    sentiment = row[20]
    i=i+1
    processedTweet = processTweet(tweet)
    featureVector = getFeatureVector(processedTweet, stopWords)
    featureList.extend(featureVector)
    tweets.append((featureVector, sentiment));

# Remove featureList duplicates
featureList = list(set(featureList))
# Extract feature vector for all tweets in one shote
tweets_set = nltk.classify.util.apply_features(extract_features, tweets)

training_set, testing_set = train_test_split(tweets_set, test_size = 0.25)

# Train the classifier
NBClassifier = nltk.NaiveBayesClassifier.train(training_set)



print("Naive Bayes Classifier accuracy percent:",(nltk.classify.accuracy(NBClassifier, testing_set))*100)


MaxEntClassifier = nltk.classify.maxent.MaxentClassifier.train(training_set, 'GIS', trace=3,
                    encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 2)

print("Max Entropy Classifier accuracy percent:",(nltk.classify.accuracy(MaxEntClassifier, testing_set))*100)


